{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6acc1348-78a0-4817-a47f-507b0b1fd95e",
   "metadata": {
    "tags": []
   },
   "source": [
    "\"\"\"\n",
    "Improved Disease Prediction using Microbiome Data\n",
    "\n",
    "Key Improvements:\n",
    "1. XGBoost-only approach (removed PyCaret)\n",
    "2. Both PCA and UMAP visualizations\n",
    "3. Enhanced feature selection with multiple methods\n",
    "4. Class balancing via random subsampling of healthy controls\n",
    "5. Cross-validation with stratified folds\n",
    "6. Feature importance analysis\n",
    "7. Better evaluation metrics and visualization\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78db64af-ec21-452f-84c3-6ce20406cf56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cesc/miniconda3/envs/microbiome_classif/lib/python3.10/site-packages/umap/__init__.py:36: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.metrics import (accuracy_score, roc_auc_score, recall_score, \n",
    "                             precision_score, f1_score, cohen_kappa_score,\n",
    "                             matthews_corrcoef, confusion_matrix, classification_report)\n",
    "from scipy.stats import zscore, kruskal, mannwhitneyu\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import warnings\n",
    "import xgboost as xgb\n",
    "import umap\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1805b61-0c54-44d1-bf9b-803f34b66a19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MICROBIOME DISEASE CLASSIFICATION - IMPROVED VERSION\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MICROBIOME DISEASE CLASSIFICATION - IMPROVED VERSION\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cffa5fdf-e316-481f-874b-3d49b412432e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taxonomy table download finished\n",
      "sample table download finished\n",
      "sample run download finished\n",
      "abundance table download finished\n"
     ]
    }
   ],
   "source": [
    "# Download abundance data from GMrepo\n",
    "\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "# List of files to download with their URLs\n",
    "files_to_download = [\n",
    "    (\"superkingdom2descendents.txt.gz\", \"https://gmrepo.humangut.info/Downloads/SQLDumps/superkingdom2descendents.txt.gz\", \"taxonomy table\"),\n",
    "    (\"samples_loaded.txt.gz\", \"https://gmrepo.humangut.info/Downloads/SQLDumps/samples_loaded.txt.gz\", \"sample table\"),\n",
    "    (\"sample_to_run_info.txt.gz\", \"https://gmrepo.humangut.info/Downloads/SQLDumps/sample_to_run_info.txt.gz\", \"sample run\"),\n",
    "    (\"species_abundance.txt.gz\", \"https://gmrepo.humangut.info/Downloads/SQLDumps/species_abundance.txt.gz\", \"abundance table\")\n",
    "]\n",
    "\n",
    "for filename, url, description in files_to_download:\n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"Downloading {description}...\")\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "        print(f\"{description} download finished\")\n",
    "    else:\n",
    "        print(f\"{filename} already exists, skipping download\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8655f44a-9c98-4838-911f-64e0fdb846be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 1. DATA LOADING\n",
    "# ============================================================================\n",
    "print(\"\\n[1/8] Loading data...\")\n",
    "\n",
    "abundance_raw = pd.read_table(\"species_abundance.txt\")\n",
    "taxonomy_table = pd.read_table(\"superkingdom2descendents.txt\")\n",
    "sample_table = pd.read_table(\"samples_loaded.txt\")\n",
    "run_table = pd.read_table(\"sample_to_run_info.txt\", dtype='str')\n",
    "\n",
    "print(f\"✓ Loaded {len(abundance_raw)} abundance records\")\n",
    "print(f\"✓ Loaded {len(taxonomy_table)} taxonomy entries\")\n",
    "print(f\"✓ Loaded {len(sample_table)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3de069-1650-4af7-8930-b328fa869b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 2. DATA PREPROCESSING\n",
    "# ============================================================================\n",
    "print(\"\\n[2/8] Preprocessing abundance data...\")\n",
    "\n",
    "# Filter to genus level\n",
    "abundance_genus = abundance_raw[abundance_raw['taxon_rank_level'].str.contains('genus', case=False)]\n",
    "\n",
    "# Pivot table: samples as rows, taxa as columns\n",
    "pivoted_df = abundance_genus.pivot_table(\n",
    "    index='loaded_uid', \n",
    "    columns='ncbi_taxon_id', \n",
    "    values='relative_abundance', \n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "# Rename columns with genus names\n",
    "taxonomy_table['scientific_name'] = taxonomy_table['scientific_name'].str.replace(' ', '_')\n",
    "mapping = dict(zip(taxonomy_table['ncbi_taxon_id'], taxonomy_table['scientific_name']))\n",
    "pivoted_df.rename(columns=mapping, inplace=True)\n",
    "\n",
    "print(f\"✓ Created abundance matrix: {pivoted_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe21c85-776e-4f18-92c5-316e019547f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 3. METADATA CLEANING\n",
    "# ============================================================================\n",
    "print(\"\\n[3/8] Cleaning metadata...\")\n",
    "\n",
    "# Merge metadata tables\n",
    "metadata_df = pd.merge(run_table, sample_table, left_on='run_id', right_on='accession_id')\n",
    "\n",
    "# Quality filters\n",
    "metadata_df = metadata_df[metadata_df['QCStatus'] != 0]\n",
    "print(f\"  - Removed {len(metadata_df[metadata_df['QCStatus'] == 0])} low-quality samples\")\n",
    "\n",
    "# Remove phenotypes with < 100 samples\n",
    "phenotype_counts = metadata_df['phenotype'].value_counts()\n",
    "phenotypes_to_keep = phenotype_counts[phenotype_counts >= 100].index\n",
    "metadata_df = metadata_df[metadata_df['phenotype'].isin(phenotypes_to_keep)]\n",
    "print(f\"  - Kept {len(phenotypes_to_keep)} phenotypes with ≥100 samples\")\n",
    "\n",
    "# Standardize phenotype names\n",
    "metadata_df['phenotype'] = metadata_df['phenotype'].replace(\n",
    "    ['healthy', 'Health', 'Normal'], 'Healthy'\n",
    ")\n",
    "metadata_df['phenotype'] = metadata_df['phenotype'].replace(\n",
    "    ['IBD', 'Inflamatory Bowel Diseases'], 'Inflamatory Bowel Disease'\n",
    ")\n",
    "\n",
    "# Remove duplicates\n",
    "non_phenotype_columns = metadata_df.columns.difference(['phenotype'])\n",
    "duplicated_rows = metadata_df.duplicated(subset=non_phenotype_columns, keep=False)\n",
    "metadata_df = metadata_df[~duplicated_rows]\n",
    "print(f\"  - Removed {duplicated_rows.sum()} duplicate rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea1c5cb-3b06-4927-a28d-2d854f4331df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 4. ABUNDANCE DATA CLEANING\n",
    "# ============================================================================\n",
    "print(\"\\n[4/8] Cleaning abundance data...\")\n",
    "\n",
    "# Keep only samples in metadata\n",
    "uids_to_keep = metadata_df[\"uid\"]\n",
    "pivoted_df_filtered = pivoted_df.loc[pivoted_df.index.isin(uids_to_keep)]\n",
    "\n",
    "# Remove unknown column (first column)\n",
    "pivoted_df_filtered = pivoted_df_filtered.iloc[:, 1:]\n",
    "\n",
    "# Verify alignment\n",
    "metadata_df = metadata_df.set_index('uid').loc[pivoted_df_filtered.index].reset_index()\n",
    "pivoted_df_filtered = pivoted_df_filtered.reset_index(drop=True)\n",
    "metadata_df = metadata_df.reset_index(drop=True)\n",
    "\n",
    "print(f\"✓ Final dataset: {len(pivoted_df_filtered)} samples × {len(pivoted_df_filtered.columns)} features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dff1b3-1ff0-4163-b966-42e3e855b8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 5. HEALTHY SAMPLES - OUTLIER REMOVAL\n",
    "# ============================================================================\n",
    "print(\"\\n[5/8] Processing healthy samples and removing outliers...\")\n",
    "\n",
    "# Subset healthy samples\n",
    "healthy_metadata = metadata_df[metadata_df[\"phenotype\"] == \"Healthy\"].copy()\n",
    "pivoted_df_Healthy = pivoted_df_filtered.loc[healthy_metadata.index].copy()\n",
    "\n",
    "# Standardize and perform PCA\n",
    "scaler = StandardScaler()\n",
    "healthy_standardized = scaler.fit_transform(pivoted_df_Healthy)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(healthy_standardized)\n",
    "pca_df = pd.DataFrame(data=pca_result, columns=['PC1', 'PC2'])\n",
    "\n",
    "# Identify outliers using Z-scores\n",
    "z_scores = zscore(pca_df)\n",
    "outlier_threshold = 3\n",
    "outliers = (np.abs(z_scores) > outlier_threshold).any(axis=1)\n",
    "\n",
    "print(f\"  - Identified {outliers.sum()} outliers in healthy samples\")\n",
    "\n",
    "# Remove outliers\n",
    "pivoted_df_Healthy = pivoted_df_Healthy[~outliers].reset_index(drop=True)\n",
    "healthy_metadata = healthy_metadata[~outliers].reset_index(drop=True)\n",
    "\n",
    "print(f\"✓ Clean healthy samples: {len(pivoted_df_Healthy)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739d26cf-d7e8-47ba-8bf4-a67a11a36a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 6. VISUALIZATION - HEALTHY SAMPLES (PCA + UMAP)\n",
    "# ============================================================================\n",
    "print(\"\\n[6/8] Creating visualizations...\")\n",
    "\n",
    "# Re-standardize after outlier removal\n",
    "healthy_standardized = scaler.fit_transform(pivoted_df_Healthy)\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(healthy_standardized)\n",
    "\n",
    "# UMAP\n",
    "print(\"  - Computing UMAP (this may take a few minutes)...\")\n",
    "umap_reducer = umap.UMAP(n_components=2, random_state=RANDOM_SEED, n_neighbors=15, min_dist=0.1)\n",
    "umap_result = umap_reducer.fit_transform(healthy_standardized)\n",
    "\n",
    "# Plot both\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# PCA plot\n",
    "axes[0].scatter(pca_result[:, 0], pca_result[:, 1], alpha=0.5, s=10)\n",
    "axes[0].set_title('PCA of Healthy Samples (Outliers Removed)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel(f'PC1: {pca.explained_variance_ratio_[0]*100:.2f}%')\n",
    "axes[0].set_ylabel(f'PC2: {pca.explained_variance_ratio_[1]*100:.2f}%')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# UMAP plot\n",
    "axes[1].scatter(umap_result[:, 0], umap_result[:, 1], alpha=0.5, s=10)\n",
    "axes[1].set_title('UMAP of Healthy Samples (Outliers Removed)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('UMAP 1')\n",
    "axes[1].set_ylabel('UMAP 2')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('healthy_samples_pca_umap.png', dpi=300, bbox_inches='tight')\n",
    "print(\"✓ Saved: healthy_samples_pca_umap.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81af7428-00d2-4345-b90a-fbf2f7b2f2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 7. DISEASE CLASSIFICATION LOOP\n",
    "# ============================================================================\n",
    "print(\"\\n[7/8] Training disease classifiers...\")\n",
    "\n",
    "# Separate disease samples\n",
    "non_healthy_metadata = metadata_df[metadata_df[\"phenotype\"] != \"Healthy\"].reset_index(drop=True)\n",
    "pivoted_df_non_Healthy = pivoted_df_filtered.loc[metadata_df[metadata_df[\"phenotype\"] != \"Healthy\"].index].reset_index(drop=True)\n",
    "\n",
    "# Storage for results\n",
    "all_results = []\n",
    "feature_importance_dict = {}\n",
    "\n",
    "# Get list of diseases\n",
    "diseases = non_healthy_metadata['phenotype'].unique()\n",
    "print(f\"\\nProcessing {len(diseases)} diseases...\")\n",
    "\n",
    "for disease in tqdm(diseases, desc=\"Disease classification\"):\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Processing: {disease}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 7.1 Subset disease samples\n",
    "    # ========================================================================\n",
    "    is_disease = non_healthy_metadata['phenotype'] == disease\n",
    "    disease_data = pivoted_df_non_Healthy[is_disease].copy()\n",
    "    disease_labels = non_healthy_metadata[is_disease].copy()\n",
    "    \n",
    "    n_disease = len(disease_data)\n",
    "    n_healthy = len(pivoted_df_Healthy)\n",
    "    \n",
    "    print(f\"  Disease samples: {n_disease}\")\n",
    "    print(f\"  Healthy samples: {n_healthy}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 7.2 Balance classes by subsampling healthy controls\n",
    "    # ========================================================================\n",
    "    # Randomly subsample healthy samples to match disease sample count\n",
    "    healthy_indices = np.random.choice(\n",
    "        pivoted_df_Healthy.index, \n",
    "        size=n_disease, \n",
    "        replace=False\n",
    "    )\n",
    "    \n",
    "    balanced_healthy_data = pivoted_df_Healthy.loc[healthy_indices].copy()\n",
    "    balanced_healthy_labels = healthy_metadata.loc[healthy_indices].copy()\n",
    "    \n",
    "    print(f\"  Balanced healthy samples: {len(balanced_healthy_data)}\")\n",
    "    print(f\"  ✓ Classes balanced (1:1 ratio)\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 7.3 Feature selection using multiple methods\n",
    "    # ========================================================================\n",
    "    print(\"  Performing feature selection...\")\n",
    "    \n",
    "    # Method 1: Kruskal-Wallis test (univariate non-parametric)\n",
    "    kw_features = []\n",
    "    kw_pvalues = []\n",
    "    \n",
    "    for col in pivoted_df_Healthy.columns:\n",
    "        # Check for sufficient variance\n",
    "        if len(balanced_healthy_data[col].unique()) > 1 and len(disease_data[col].unique()) > 1:\n",
    "            _, p_value = kruskal(balanced_healthy_data[col], disease_data[col])\n",
    "            if p_value < 0.05:\n",
    "                kw_features.append(col)\n",
    "                kw_pvalues.append(p_value)\n",
    "    \n",
    "    # Method 2: Mann-Whitney U test (more robust for small samples)\n",
    "    mw_features = []\n",
    "    mw_pvalues = []\n",
    "    \n",
    "    for col in pivoted_df_Healthy.columns:\n",
    "        if len(balanced_healthy_data[col].unique()) > 1 and len(disease_data[col].unique()) > 1:\n",
    "            _, p_value = mannwhitneyu(balanced_healthy_data[col], disease_data[col], alternative='two-sided')\n",
    "            if p_value < 0.05:\n",
    "                mw_features.append(col)\n",
    "                mw_pvalues.append(p_value)\n",
    "    \n",
    "    # Combine features from both methods (union)\n",
    "    selected_features = list(set(kw_features) | set(mw_features))\n",
    "    \n",
    "    # If too many features, select top N by combined p-value ranking\n",
    "    MAX_FEATURES = 500\n",
    "    if len(selected_features) > MAX_FEATURES:\n",
    "        # Create combined p-value ranking\n",
    "        feature_scores = {}\n",
    "        for feat in selected_features:\n",
    "            kw_p = kw_pvalues[kw_features.index(feat)] if feat in kw_features else 1.0\n",
    "            mw_p = mw_pvalues[mw_features.index(feat)] if feat in mw_features else 1.0\n",
    "            feature_scores[feat] = min(kw_p, mw_p)  # Take best p-value\n",
    "        \n",
    "        # Select top features\n",
    "        selected_features = sorted(feature_scores.keys(), key=lambda x: feature_scores[x])[:MAX_FEATURES]\n",
    "    \n",
    "    print(f\"  Selected features: {len(selected_features)}\")\n",
    "    \n",
    "    # If no significant features found, use top 100 by variance\n",
    "    if len(selected_features) < 10:\n",
    "        print(\"  Warning: Few significant features found, using top variance features\")\n",
    "        combined_data = pd.concat([balanced_healthy_data, disease_data])\n",
    "        variances = combined_data.var()\n",
    "        selected_features = variances.nlargest(100).index.tolist()\n",
    "    \n",
    "    # Filter data to selected features\n",
    "    balanced_healthy_filtered = balanced_healthy_data[selected_features].copy()\n",
    "    disease_filtered = disease_data[selected_features].copy()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 7.4 Combine and prepare data\n",
    "    # ========================================================================\n",
    "    X = pd.concat([balanced_healthy_filtered, disease_filtered]).reset_index(drop=True)\n",
    "    y_labels = pd.concat([balanced_healthy_labels, disease_labels])['phenotype'].reset_index(drop=True)\n",
    "    y = (y_labels != \"Healthy\").astype(int)  # Binary: 0=Healthy, 1=Disease\n",
    "    \n",
    "    print(f\"  Final dataset shape: {X.shape}\")\n",
    "    print(f\"  Class distribution: Healthy={sum(y==0)}, {disease}={sum(y==1)}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 7.5 Visualizations (PCA + UMAP)\n",
    "    # ========================================================================\n",
    "    scaler_viz = StandardScaler()\n",
    "    X_scaled = scaler_viz.fit_transform(X)\n",
    "    \n",
    "    # PCA\n",
    "    pca_viz = PCA(n_components=2)\n",
    "    pca_coords = pca_viz.fit_transform(X_scaled)\n",
    "    \n",
    "    # UMAP\n",
    "    umap_viz = umap.UMAP(n_components=2, random_state=RANDOM_SEED, n_neighbors=15, min_dist=0.1)\n",
    "    umap_coords = umap_viz.fit_transform(X_scaled)\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    colors = ['#2E86AB' if label == 0 else '#A23B72' for label in y]\n",
    "    labels_str = ['Healthy' if label == 0 else disease for label in y]\n",
    "    \n",
    "    # PCA\n",
    "    for label_val, label_str, color in zip([0, 1], ['Healthy', disease], ['#2E86AB', '#A23B72']):\n",
    "        mask = y == label_val\n",
    "        axes[0].scatter(pca_coords[mask, 0], pca_coords[mask, 1], \n",
    "                       c=color, label=label_str, alpha=0.6, s=30)\n",
    "    axes[0].set_title(f'PCA: Healthy vs {disease}', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xlabel(f'PC1: {pca_viz.explained_variance_ratio_[0]*100:.2f}%')\n",
    "    axes[0].set_ylabel(f'PC2: {pca_viz.explained_variance_ratio_[1]*100:.2f}%')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # UMAP\n",
    "    for label_val, label_str, color in zip([0, 1], ['Healthy', disease], ['#2E86AB', '#A23B72']):\n",
    "        mask = y == label_val\n",
    "        axes[1].scatter(umap_coords[mask, 0], umap_coords[mask, 1], \n",
    "                       c=color, label=label_str, alpha=0.6, s=30)\n",
    "    axes[1].set_title(f'UMAP: Healthy vs {disease}', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xlabel('UMAP 1')\n",
    "    axes[1].set_ylabel('UMAP 2')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save with safe filename\n",
    "    safe_disease_name = disease.replace('/', '_').replace(' ', '_')\n",
    "    plt.savefig(f'{safe_disease_name}_pca_umap.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # 7.6 XGBoost Training with Cross-Validation\n",
    "    # ========================================================================\n",
    "    print(\"  Training XGBoost classifier...\")\n",
    "    \n",
    "    # Define XGBoost model with optimal parameters\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=RANDOM_SEED,\n",
    "        eval_metric='logloss',\n",
    "        use_label_encoder=False,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "    \n",
    "    scoring = {\n",
    "        'accuracy': 'accuracy',\n",
    "        'roc_auc': 'roc_auc',\n",
    "        'recall': 'recall',\n",
    "        'precision': 'precision',\n",
    "        'f1': 'f1'\n",
    "    }\n",
    "    \n",
    "    cv_results = cross_validate(\n",
    "        xgb_model, X, y, cv=cv, scoring=scoring, \n",
    "        return_train_score=False, n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Train final model on full data\n",
    "    xgb_model.fit(X, y)\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred = xgb_model.predict(X)\n",
    "    y_pred_proba = xgb_model.predict_proba(X)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    results = {\n",
    "        'Disease': disease,\n",
    "        'N_Disease': n_disease,\n",
    "        'N_Healthy_Balanced': len(balanced_healthy_data),\n",
    "        'N_Features': len(selected_features),\n",
    "        'CV_Accuracy_Mean': cv_results['test_accuracy'].mean(),\n",
    "        'CV_Accuracy_Std': cv_results['test_accuracy'].std(),\n",
    "        'CV_ROC_AUC_Mean': cv_results['test_roc_auc'].mean(),\n",
    "        'CV_ROC_AUC_Std': cv_results['test_roc_auc'].std(),\n",
    "        'CV_Recall_Mean': cv_results['test_recall'].mean(),\n",
    "        'CV_Recall_Std': cv_results['test_recall'].std(),\n",
    "        'CV_Precision_Mean': cv_results['test_precision'].mean(),\n",
    "        'CV_Precision_Std': cv_results['test_precision'].std(),\n",
    "        'CV_F1_Mean': cv_results['test_f1'].mean(),\n",
    "        'CV_F1_Std': cv_results['test_f1'].std(),\n",
    "        'Train_Accuracy': accuracy_score(y, y_pred),\n",
    "        'Train_ROC_AUC': roc_auc_score(y, y_pred_proba),\n",
    "        'Train_Recall': recall_score(y, y_pred),\n",
    "        'Train_Precision': precision_score(y, y_pred),\n",
    "        'Train_F1': f1_score(y, y_pred),\n",
    "        'Train_Kappa': cohen_kappa_score(y, y_pred),\n",
    "        'Train_MCC': matthews_corrcoef(y, y_pred)\n",
    "    }\n",
    "    \n",
    "    all_results.append(results)\n",
    "    \n",
    "    # Store feature importance\n",
    "    feature_importance_dict[disease] = pd.DataFrame({\n",
    "        'feature': selected_features,\n",
    "        'importance': xgb_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"  ✓ CV ROC-AUC: {results['CV_ROC_AUC_Mean']:.4f} ± {results['CV_ROC_AUC_Std']:.4f}\")\n",
    "    print(f\"  ✓ CV F1: {results['CV_F1_Mean']:.4f} ± {results['CV_F1_Std']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463cb135-b338-48ba-a999-0ad49745721e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 8. RESULTS ANALYSIS\n",
    "# ============================================================================\n",
    "print(\"\\n[8/8] Analyzing results...\")\n",
    "\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv('xgboost_classification_results.csv', index=False)\n",
    "print(\"✓ Saved: xgboost_classification_results.csv\")\n",
    "\n",
    "# Filter for good models (CV ROC-AUC > 0.70 and CV F1 > 0.60)\n",
    "good_models = results_df[\n",
    "    (results_df['CV_ROC_AUC_Mean'] > 0.70) & \n",
    "    (results_df['CV_F1_Mean'] > 0.60)\n",
    "].copy()\n",
    "\n",
    "print(f\"\\n✓ Found {len(good_models)} diseases with good classification performance\")\n",
    "\n",
    "if len(good_models) > 0:\n",
    "    # Sort by ROC-AUC\n",
    "    good_models = good_models.sort_values('CV_ROC_AUC_Mean', ascending=False)\n",
    "    \n",
    "    # Display top results\n",
    "    print(\"\\nTop 10 Best Classified Diseases:\")\n",
    "    print(\"=\"*80)\n",
    "    display_cols = ['Disease', 'CV_ROC_AUC_Mean', 'CV_F1_Mean', 'CV_Precision_Mean', 'CV_Recall_Mean']\n",
    "    print(good_models[display_cols].head(10).to_string(index=False))\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Visualization: Performance metrics\n",
    "    # ========================================================================\n",
    "    top_n = min(15, len(good_models))\n",
    "    top_models = good_models.head(top_n)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # ROC-AUC\n",
    "    axes[0, 0].barh(range(len(top_models)), top_models['CV_ROC_AUC_Mean'], \n",
    "                     xerr=top_models['CV_ROC_AUC_Std'], color='steelblue', alpha=0.8)\n",
    "    axes[0, 0].set_yticks(range(len(top_models)))\n",
    "    axes[0, 0].set_yticklabels(top_models['Disease'], fontsize=9)\n",
    "    axes[0, 0].set_xlabel('ROC-AUC Score', fontweight='bold')\n",
    "    axes[0, 0].set_title('Cross-Validation ROC-AUC', fontweight='bold')\n",
    "    axes[0, 0].grid(axis='x', alpha=0.3)\n",
    "    axes[0, 0].axvline(0.8, color='red', linestyle='--', alpha=0.5, label='0.8 threshold')\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # F1 Score\n",
    "    axes[0, 1].barh(range(len(top_models)), top_models['CV_F1_Mean'], \n",
    "                     xerr=top_models['CV_F1_Std'], color='coral', alpha=0.8)\n",
    "    axes[0, 1].set_yticks(range(len(top_models)))\n",
    "    axes[0, 1].set_yticklabels(top_models['Disease'], fontsize=9)\n",
    "    axes[0, 1].set_xlabel('F1 Score', fontweight='bold')\n",
    "    axes[0, 1].set_title('Cross-Validation F1 Score', fontweight='bold')\n",
    "    axes[0, 1].grid(axis='x', alpha=0.3)\n",
    "    axes[0, 1].axvline(0.7, color='red', linestyle='--', alpha=0.5, label='0.7 threshold')\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # Precision\n",
    "    axes[1, 0].barh(range(len(top_models)), top_models['CV_Precision_Mean'], \n",
    "                     xerr=top_models['CV_Precision_Std'], color='seagreen', alpha=0.8)\n",
    "    axes[1, 0].set_yticks(range(len(top_models)))\n",
    "    axes[1, 0].set_yticklabels(top_models['Disease'], fontsize=9)\n",
    "    axes[1, 0].set_xlabel('Precision', fontweight='bold')\n",
    "    axes[1, 0].set_title('Cross-Validation Precision', fontweight='bold')\n",
    "    axes[1, 0].grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Recall\n",
    "    axes[1, 1].barh(range(len(top_models)), top_models['CV_Recall_Mean'], \n",
    "                     xerr=top_models['CV_Recall_Std'], color='purple', alpha=0.8)\n",
    "    axes[1, 1].set_yticks(range(len(top_models)))\n",
    "    axes[1, 1].set_yticklabels(top_models['Disease'], fontsize=9)\n",
    "    axes[1, 1].set_xlabel('Recall', fontweight='bold')\n",
    "    axes[1, 1].set_title('Cross-Validation Recall', fontweight='bold')\n",
    "    axes[1, 1].grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('XGBoost Classification Performance - Top Diseases', \n",
    "                 fontsize=16, fontweight='bold', y=1.00)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('classification_performance_summary.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"✓ Saved: classification_performance_summary.png\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Feature importance for top disease\n",
    "    # ========================================================================\n",
    "    if len(good_models) > 0:\n",
    "        top_disease = good_models.iloc[0]['Disease']\n",
    "        top_features = feature_importance_dict[top_disease].head(20)\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.barh(range(len(top_features)), top_features['importance'], color='darkblue', alpha=0.7)\n",
    "        plt.yticks(range(len(top_features)), top_features['feature'], fontsize=9)\n",
    "        plt.xlabel('Feature Importance', fontweight='bold')\n",
    "        plt.title(f'Top 20 Features for {top_disease}', fontweight='bold', fontsize=14)\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.grid(axis='x', alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'feature_importance_{top_disease.replace(\"/\", \"_\").replace(\" \", \"_\")}.png', \n",
    "                    dpi=300, bbox_inches='tight')\n",
    "        print(f\"✓ Saved: feature importance plot for {top_disease}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9693471-ec15-4ded-a34a-118ec204043e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nKey Files Generated:\")\n",
    "print(\"  1. xgboost_classification_results.csv - Complete results for all diseases\")\n",
    "print(\"  2. healthy_samples_pca_umap.png - Visualization of healthy cohort\")\n",
    "print(\"  3. [disease]_pca_umap.png - PCA/UMAP for each disease\")\n",
    "print(\"  4. classification_performance_summary.png - Performance metrics overview\")\n",
    "print(\"  5. feature_importance_*.png - Top features for best disease\")\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(f\"  Total diseases analyzed: {len(results_df)}\")\n",
    "print(f\"  Diseases with good performance (AUC>0.7, F1>0.6): {len(good_models)}\")\n",
    "if len(good_models) > 0:\n",
    "    print(f\"  Best disease: {good_models.iloc[0]['Disease']}\")\n",
    "    print(f\"  Best ROC-AUC: {good_models.iloc[0]['CV_ROC_AUC_Mean']:.4f}\")\n",
    "    print(f\"  Best F1: {good_models.iloc[0]['CV_F1_Mean']:.4f}\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
